%!TEX root = htm.tex
\section{Introduction}
\label{sec:intro}
%
%
The \emph{Transactional Memory (TM)} abstraction is a synchronization mechanism 
that allows the programmer to \emph{optimistically} execute sequences of shared-memory
operations as \emph{atomic transactions}.
Several software TM designs~\cite{norec, ST95,HLM+03,fraser} have been introduced subsequent to the original TM proposal based in
hardware~\cite{HM93}. 
The original dynamic STM implementation DSTM~\cite{HLM+03} ensures that a transaction aborts only if there is a read-write \emph{data conflict} with a concurrent
transaction (\`a la \emph{progressiveness}~\cite{tm-book}). However, to satisfy \emph{opacity}~\cite{tm-book}, read operations in DSTM must \emph{incrementally} validate
the responses of all previous read operations to avoid inconsistent executions. 
This results in quadratic  (in the size of the transaction's read
set) step-complexity for transactions. Subsequent STM 
implementations like NOrec~\cite{norec} and TL2~\cite{DSS06}
minimize the impact on performance due to incremental validation.
NOrec uses a global sequence lock that is read at the start of a transaction and performs \emph{value-based}
validation during read operations only if the value of the global lock has been changed (by an updating transaction) 
since reading it.
TL2, on the other hand, eliminates incremental validation completely.
Like NOrec, it uses a global sequence lock, but each data item also 
has an associated sequence lock value that is updated alongside the data item.
When a data item is read, if its associated sequence lock value is different 
from the value that was read from the sequence lock at the start of the transaction, then the transaction aborts.

In fact, STMs like TL2 and NOrec ensure progress in the absence of data conflicts with 
O(1) step complexity read operations and \emph{invisible reads} (read operations which 
do not modify shared memory).
Nonetheless, TM designs that are implemented entirely in software still incur significant performance overhead.
Thus, current CPUs have included instructions to mark a block of memory accesses as transactional~\cite{asf, bluegene}, allowing them to be executed \emph{atomically} in hardware.
Hardware transactions promise better performance than STMs, but they offer no progress guarantees 
since they may experience \emph{spurious} aborts. This motivates the need for
\emph{hybrid} TMs in which the \emph{fast} hardware transactions are 
complemented with \emph{slower} software transactions that do not have spurious aborts.

To allow hardware transactions in a HyTM to detect conflicts with software transactions, hardware transactions must be \emph{instrumented} to perform additional metadata accesses, which introduces overhead.
Hardware transactions typically provide automatic conflict detection at cacheline granularity,
thus ensuring that a transaction will be aborted if it experiences memory contention on a cacheline.
This is at least the case with Intel's Transactional Synchronization Extensions~\cite{haswell}.
The IBM POWER8 architecture additionally allows hardware transactions to access metadata \emph{non-speculatively}, 
thus bypassing automatic conflict detection. While this has the advantage of potentially reducing contention aborts
in hardware, this makes the design of HyTM implementations potentially harder to prove correct.

In \cite{hytm14disc}, it was shown that hardware transactions in opaque progressive HyTMs must perform
at least one metadata access per transactional read and write.
In this paper, we show that in opaque progressive HyTMs with invisible reads, 
software transactions \textit{cannot} avoid incremental validation.
Specifically, we prove that \textit{each read operation} of a software transaction in a progressive HyTM
must necessarily incur a validation cost that is \emph{linear} 
in the size of the transaction's read set. 
This is in contrast to TL2 which is progressive and has constant complexity read operations.
Thus, in addition to the linear instrumentation cost in hardware transactions, there is a quadratic step complexity cost in software transactions.

We then present opaque HyTM algorithms providing \emph{progressiveness for a subset of transactions} that are  %both hardware and software transactions \trevor{maybe just ``transactions''?}
optimal in terms of hardware instrumentation.
Algorithm~1 is progressive for all transactions, but it incurs high instrumentation overhead in practice.
Algorithm~2 avoids all instrumentation in fast-path read operations, but is progressive only for slow-path reading transactions.
We also sketch how \emph{some} hardware instrumentation can be performed \textit{non-speculatively} without violating opacity.

Extensive experiments were performed to characterize the \textit{cost of concurrency} in practice.
We studied the instrumentation-optimal algorithms, as well as TL2, Transactional Lock Elision (TLE)~\cite{tle} and Hybrid NOrec~\cite{hynorecriegel} on both Intel and IBM POWER architectures.
Each of the algorithms we studied contributes to an improved understanding of the concurrency vs. hardware instrumentation vs. software validation trade-offs for HyTMs.
Comparing results between the very different Intel and IBM POWER architectures also led to new insights.
%
Collectively, our results suggest the following.
(i) The \emph{cost of concurrency} is significant in practice; high hardware instrumentation impacts performance negatively on Intel and much more so on POWER8 due to its limited transactional cache capacity.
(ii) It is important to implement HyTMs that provide progressiveness for a maximal set of transactions without incurring high hardware instrumentation overhead or using global contending bottlenecks.
(iii) There is no easy way to derive more efficient HyTMs by taking advantage of non-speculative accesses supported within the fast-path in POWER8 processors. %(but not in Intel HTMs).

%